# Running Gradient Descent for Linear Regression

We will observe the gradient descent algorithm in action, specifically for fitting a linear regression model.

## Visualization of the Algorithm
- **Plots**:
  - Upper left: Plot of the model and training data.
  - Upper right: Contour plot of the cost function.
  - Bottom: Surface plot of the cost function.

  ![alt text](<res/Screenshot from 2024-11-02 18-16-48.png>)

### Initial Setup
- Initial values:
  - $ w = -0.1 $
  - $ b = 900 $
- Corresponding function:
  $$ f(x) = -0.1x + 900 $$

### Step-by-Step Gradient Descent
1. **First Step**:
   - After the first update, the cost function moves down and to the right, indicating a decrease in error.
   - The straight line fit also changes slightly.

2. **Subsequent Steps**:
   - Each additional step continues to decrease the cost.
   - Parameters $ w $ and $ b $ follow a trajectory that leads to a better fitting line.
   - The straight line fit improves with each iteration until it reaches a good fit.

### Global Minimum
- The process continues until reaching the global minimum, which corresponds to the best fit for the training data.
- Example Prediction:
  - For a house size of 1250 square feet, the model can predict a value (e.g., $250,000).

## Batch Gradient Descent
![alt text](<res/Screenshot from 2024-11-02 18-19-25.png>)
- The process used here is called **batch gradient descent**:
  - It calculates gradients using all training examples in each update step.
- Definition:
  - "Batch" refers to considering the entire training set for derivative calculations.

## Other Gradient Descent Variants
- Other versions of gradient descent consider smaller subsets of training data (stochastic gradient descent), but for this lesson, we focus on batch gradient descent.
