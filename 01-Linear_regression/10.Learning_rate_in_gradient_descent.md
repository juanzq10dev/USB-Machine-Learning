# Understanding the Learning Rate in Gradient Descent

The choice of the learning rate, $\alpha$, significantly impacts the efficiency of gradient descent. A poorly chosen learning rate can lead to non-functional descent.

## Gradient Descent Update Rule
The rule for updating $w$ is given by:

$$
w \gets w - \alpha \frac{d}{dw} J(w)
$$

## Effects of Learning Rate on Gradient Descent

![alt text](<res/Screenshot from 2024-11-02 17-46-49.png>)

### 1. Learning Rate Too Small
- **Graph Setup**: The horizontal axis is $w$, and the vertical axis is the cost $J(w)$.
- **Process**:
  - Starting at a certain point, the derivative term is multiplied by a very small $\alpha$ (e.g., $0.0000001$).
  - Updates result in tiny steps toward the minimum.
- **Outcome**:
  - Cost $J(w)$ decreases but very slowly.
  - Requires many steps to approach the minimum, leading to inefficiency.
  
**Summary**: With a too-small learning rate, gradient descent works, but it is painfully slow.

### 2. Learning Rate Too Large
- **Graph Setup**: Similar cost function graph.
- **Process**:
  - Starting close to the minimum, a large $\alpha$ causes a significant update.
  - The new $w$ moves far away from the minimum, increasing the cost.
- **Outcome**:
  - May overshoot the minimum, causing divergence rather than convergence.

**Summary**: A too-large learning rate can cause gradient descent to overshoot and fail to converge.

## Local Minimum Scenario
- If $w$ is already at a local minimum:
  - The derivative at this point is zero, leading to the update:
    $$
    w \gets w - \alpha \times 0
    $$
  - Thus, $w$ remains unchanged, which is desirable.

![alt text](<res/Screenshot from 2024-11-02 17-48-46.png>)

### Key Insight
- Gradient descent will not alter $w$ if it's already at a local minimum, maintaining stability in the solution.

## Illustration of Gradient Descent Steps
- As gradient descent approaches a local minimum:
  - Initial steps may be large, but as $w$ moves closer to the minimum, the slope of the cost function (derivative) decreases.
  - This results in smaller updates with each iteration, even if $\alpha$ remains constant.

  ![alt text](<res/Screenshot from 2024-11-02 17-50-15.png>)

**Conclusion**: As $w$ nears a local minimum, gradient descent automatically takes smaller steps due to the diminishing derivative.

## Next Steps
- The next video will apply this understanding to linear regression, revisiting the mean squared error cost function as the basis for our learning algorithm.
