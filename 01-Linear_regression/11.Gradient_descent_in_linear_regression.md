# Implementing Gradient Descent with Linear Regression

We will integrate the squared error cost function for linear regression with the gradient descent algorithm, allowing us to fit a straight line to our training data.

## Overview
- Review of:
  - Linear regression model
  - Squared error cost function
  - Gradient descent algorithm

## Derivatives for Gradient Descent
### Derivatives Calculation
- **Derivative with respect to $w $**:
  $$
  \frac{dJ}{dw} = \frac{1}{m} \sum_{i=1}^{m} {(f_{w, b} (x^i) - y^i)}\times x_i
  $$
- **Derivative with respect to $b $**:
  $$
  \frac{dJ}{db} = \frac{1}{m} \sum_{i=1}^{m} {(f_{w, b} (x^i) - y^i)}
  $$
  (no $x_i $ term at the end)

### Derivation Details
- The derivatives are derived using calculus.
- If unfamiliar with calculus, you can skip the derivation and still successfully implement gradient descent.

![alt text](<res/Screenshot from 2024-11-02 18-06-25.png>)

#### Derivation Breakdown
1. **Cost Function**:
   - The squared error cost function $J $ is defined as:
     $$
     J(w, b) = \frac{1}{2m} \sum_{i=1}^{m} (f(w, b, x^i) - y^i)^2
     $$
   - Where $ f(w, b, x^i) = wx^i + b $.
  
2. **Derivative with respect to $w $**:
   - By applying the rules of calculus, the 2's in the squared term cancel, leading to:
     $$
     \frac{dJ}{dw} = \frac{1}{m} \sum_{i=1}^{m} (f(w, b, x^i) - y^i) \cdot x^i
     $$

3. **Derivative with respect to $b $**:
   - Following a similar process, the derivative simplifies to:
     $$
     \frac{dJ}{db} = \frac{1}{m} \sum_{i=1}^{m} (f(w, b, x^i) - y^i)
     $$

## Gradient Descent Algorithm for Linear Regression
- Repeatedly update $w $ and $b $ using the derived expressions until convergence:
  $$
  w \gets w - \alpha \frac{dJ}{dw}
  $$
  $$
  b \gets b - \alpha \frac{dJ}{db}
  $$

  ![alt text](<res/Screenshot from 2024-11-02 18-07-18.png>)

## Key Concepts
### Local vs. Global Minimum
- Gradient descent may lead to a local minimum, but:
  - For linear regression with the squared error cost function, the cost function is convex and has a single global minimum.
  - Convex functions are bowl-shaped and do not have local minima.

  ![alt text](<res/Screenshot from 2024-11-02 18-08-14.png>)

### Convergence
- When using gradient descent on a convex function with an appropriate learning rate, it will always converge to the global minimum.

## Conclusion
- You now understand how to implement gradient descent for linear regression.
- The next video will demonstrate this algorithm in action.
