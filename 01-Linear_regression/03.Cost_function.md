# Cost Function in Linear Regression

- Tells us how well the model is doing.

### Key concepts
- **Training set:** Contains input features $x$ and output targets $y$. (The table set)
- **Model function:** The model used to fit the data is a linear function: $f_{w,b}(x) = wx + b$.
    - **parameters $w$ and $b$:** 
       - Other names for $w$ and $b$: *parameters*, *coefficients* or *weights*.
      - Are adjusted during training to improve predictions.
  

### How parameters affect the model
The values of $w$ and $b$ change the line produced by the model:
1. When $w = 0$ and $b = 1.5$, the function $f(x)$ is constant, predicting $y$ as 1.5.
2. With $w = 0.5$ and $b = 0$, the function is $0.5*x$; $w$ represents the line's **slope**.
3. When $w = 0.5$ and $b = 1$, the line intersects the y-axis at $b = 1$, and $w$ still represents the slope (0.5).

![alt text](./res/cost_function_1.png)

The goal is to adjust $w$ and $b$ so that the line produced by $f(x)$ closely fits the training data, passing near the training examples.

### Predictions vs. Targets
For a given $x^i$, with an output $y^i$, the model $f(x^i) = wx^i + b$ predicts an estimated value $\hat{y}^i$, but the true target is $y^i$.

**Goal:** Find values for $w$ and $b$ that make $\hat{y}^i$ close to $y^i$ for each example in the training set.

---

## Constructing the Cost Function

The **cost function** quantifies the modelâ€™s error:
1. **Error:** $\text{Error} = \hat{y} - y$ measures the difference between prediction $\hat{y}$ and target $y$.
2. **Squared error:** $(\hat{y} - y)^2$ (squares the error to ensure non-negative values).
3. **Sum of squared errors**: across all examples: Summing these errors from $i = 1$ to $m$ (total training examples) $
\sum_{i=1}^{m} \left( \hat{y} - y \right)^2
$.
4. **Average error:** Divide the sum by $m$ to prevent the cost from scaling with the number of examples. $ \frac{1}{m}
\sum_{i=1}^{m} \left( \hat{y} - y \right)^2
$.
5. **Convenience factor $\frac{1}{2}$:** The convention divides by $2m$ to simplify later calculations, giving: 

$ \frac{1}{2m}
\sum_{i=1}^{m} \left( \hat{y} - y \right)^2
$.

Remember $ \hat y$ is the output of the model $f$, so: $f(x^i) = \hat y$, so:

$
J(w, b) = \frac{1}{2m} \sum_{i=1}^{m} \left( f(x^i) - y^i \right)^2
$

### Understanding the Cost Function $J(w, b)$
The cost function $J(w, b)$, or **squared error cost function**, is common in regression. It measures how close the predictions are to the actual values, with smaller $J$ values indicating better fits.

In machine learning, a smaller $J(w, b)$ means the model is doing a better job.
